{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>24</td><td>application_1583130900432_0025</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-6-15.ap-northeast-2.compute.internal:20888/proxy/application_1583130900432_0025/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-15-181.ap-northeast-2.compute.internal:8042/node/containerlogs/container_1583130900432_0025_01_000001/livy\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e778f17bd344eab31062f8c0cf95b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>26</td><td>application_1583130900432_0026</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//NOTE: BEFORE RUNNING ABOVE CONFIGURE STATEMENT, ENSURE THAT HUDI JARS ARE LOCATED WITHIN HDFS. \n",
    "//SEE README.MD FOR DIRECTIONS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4fe29930f44ef5a2bb941d403df0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.SaveMode\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.types.IntegerType\n",
      "import org.apache.hudi.DataSourceWriteOptions\n",
      "import org.apache.hudi.DataSourceReadOptions\n",
      "import org.apache.hudi.config.HoodieWriteConfig\n",
      "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "import org.apache.hudi.DataSourceWriteOptions\n",
    "import org.apache.hudi.DataSourceReadOptions\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6e917f5c694ec9b07fc57b0f72fde0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputBucket: String = analytics-seung-hudi-seoul\n"
     ]
    }
   ],
   "source": [
    "//Hudi Table을 저장하기 위해 앞에서 생성한 S3 버킷을 지정해줍니다. Where to store your Hudi Table.\n",
    "val outputBucket = \"analytics-seung-hudi-seoul\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5045fc66cbe34806a04ecaa915653893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputDatasetBucket: String = amazon-reviews-pds\n"
     ]
    }
   ],
   "source": [
    "//샘플 데이터로 Amazon 상품 리뷰 데이터를 사용합니다. \n",
    "val inputDatasetBucket = \"amazon-reviews-pds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c28b77177354ca7b7d399b731e5e1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiTableName: String = amazon_product_reviews\n",
      "hudiTableRecordKey: String = review_id\n",
      "hudiTablePath: String = s3://analytics-seung-hudi-seoul/createdatasets/amazon_product_reviews\n",
      "hudiTablePartitionColumn: String = review_date\n",
      "hudiTablePrecombineKey: String = timestamp\n"
     ]
    }
   ],
   "source": [
    "// Hudi option에 들어갈 기본 설정값을 입력합니다. \n",
    "//Specify common DataSourceWriteOptions in the single hudiOptions variable \n",
    "val hudiTableName = \"amazon_product_reviews\"\n",
    "val hudiTableRecordKey = \"review_id\"\n",
    "val hudiTablePath = \"s3://\" + outputBucket + \"/createdatasets/\" + hudiTableName\n",
    "val hudiTablePartitionColumn = \"review_date\"\n",
    "val hudiTablePrecombineKey = \"timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7605758f964599b1a15b098c06c8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourceData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [marketplace: string, customer_id: string ... 14 more fields]\n",
      "+-----------+--------------+-----------+--------------------+-----------+-----------+\n",
      "|marketplace|     review_id|customer_id|       product_title|star_rating|review_date|\n",
      "+-----------+--------------+-----------+--------------------+-----------+-----------+\n",
      "|         US|R3MK25NME5LTNQ|   41916184|Decorative Switch...|          5| 2014/11/17|\n",
      "|         US|R10P620FA77MVJ|   33516140|Delta 75152 2.5 G...|          5| 2009/05/12|\n",
      "|         US| RKVCGQRPHDNZ8|   17486082|Neroli 100% Pure ...|          5| 2014/11/17|\n",
      "|         US|R2KQ8CFO5FBW4F|   17359526|Traditional / Cla...|          5| 2009/05/12|\n",
      "|         US|R2NKA708V5GPGI|   15361253|Ibis & Orchid Dra...|          2| 2014/11/17|\n",
      "|         US|R2BNIIP9LQK2RG|   43316183|Lux WIN100 Heatin...|          5| 2009/05/12|\n",
      "|         US|R163IKXC5VYC50|   33660121|     MLN-55VARIATION|          5| 2014/11/17|\n",
      "|         US|R3TKASI872ESIP|   14821183|17\" Round Mexican...|          5| 2009/05/12|\n",
      "|         US| RHHUGN8XWDDJX|   18714284|  Preston Towel Ring|          2| 2014/11/17|\n",
      "|         US|R17KYB16RCQPMC|   26558176|Whirlpool EDR8D1 ...|          5| 2009/05/12|\n",
      "+-----------+--------------+-----------+--------------------+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/****************************\n",
    "Read out Amazon product reviews table\n",
    "*****************************/\n",
    "val sourceData = (spark.read.parquet(\"s3://\" + inputDatasetBucket + \"/parquet/product_category=Home_Improvement/*\")\n",
    "                            .withColumn(hudiTablePrecombineKey, current_timestamp().cast(\"long\"))\n",
    "                            .withColumn(hudiTablePartitionColumn, regexp_replace(col(hudiTablePartitionColumn), \"-\", \"/\"))\n",
    "                            .cache())\n",
    "\n",
    "sourceData.select(\"marketplace\", \"review_id\", \"customer_id\", \"product_title\", \n",
    "                  \"star_rating\", \"review_date\").show(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c5dd3b863b4344963263e80a554984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.datasource.write.precombine.field -> timestamp, hoodie.datasource.hive_sync.partition_fields -> year,month,day, hoodie.datasource.hive_sync.partition_extractor_class -> org.apache.hudi.hive.MultiPartKeysValueExtractor, hoodie.datasource.hive_sync.table -> amazon_product_reviews, hoodie.datasource.hive_sync.enable -> true, hoodie.datasource.write.recordkey.field -> review_id, hoodie.table.name -> amazon_product_reviews, hoodie.datasource.write.storage.type -> COPY_ON_WRITE, hoodie.datasource.hive_sync.assume_date_partitioning -> false, hoodie.datasource.write.partitionpath.field -> review_date)\n"
     ]
    }
   ],
   "source": [
    "/****************************\n",
    "다양한 Hudi 옵션을 지정합니다. 데이터 저장 타입과, 파티션키, 리오더키 등\n",
    "Our Hudi Options for our Product Reviews Dataset.\n",
    "*****************************/\n",
    "val hudiOptions = Map[String,String](\n",
    "  HoodieWriteConfig.TABLE_NAME -> hudiTableName,\n",
    "\n",
    "  //For this data set, we will configure it to use the COPY_ON_WRITE storage strategy. \n",
    "  //You can also choose MERGE_ON_READ\n",
    "  DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY -> \"COPY_ON_WRITE\", \n",
    "\n",
    "  //These three options configure what Hudi should use as its record key, \n",
    "  //partition column, and precombine key.\n",
    "  DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"review_id\",\n",
    "  DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> \"timestamp\",\n",
    "  DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> \"review_date\",\n",
    "\n",
    "  //For this data set, we specify that we want to sync metadata with Hive. \n",
    "  DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\",\n",
    "  DataSourceWriteOptions.HIVE_TABLE_OPT_KEY -> hudiTableName,\n",
    "  DataSourceWriteOptions.HIVE_ASSUME_DATE_PARTITION_OPT_KEY -> \"false\",\n",
    "  DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"year,month,day\",\n",
    "  DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY ->\n",
    "                                        classOf[MultiPartKeysValueExtractor].getName\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf893861eaa4ef78cac00936399eb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.apache.hudi.hive.HoodieHiveSyncException: Cannot create hive connection jdbc:hive2://localhost:10000/\n",
      "  at org.apache.hudi.hive.HoodieHiveClient.createHiveConnection(HoodieHiveClient.java:549)\n",
      "  at org.apache.hudi.hive.HoodieHiveClient.<init>(HoodieHiveClient.java:108)\n",
      "  at org.apache.hudi.hive.HiveSyncTool.<init>(HiveSyncTool.java:60)\n",
      "  at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:235)\n",
      "  at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:169)\n",
      "  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:91)\n",
      "  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "  ... 52 elided\n",
      "Caused by: java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000: java.net.ConnectException: Connection refused (Connection refused)\n",
      "  at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:232)\n",
      "  at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:176)\n",
      "  at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)\n",
      "  at java.sql.DriverManager.getConnection(DriverManager.java:664)\n",
      "  at java.sql.DriverManager.getConnection(DriverManager.java:247)\n",
      "  at org.apache.hudi.hive.HoodieHiveClient.createHiveConnection(HoodieHiveClient.java:546)\n",
      "  ... 78 more\n",
      "Caused by: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)\n",
      "  at org.apache.thrift.transport.TSocket.open(TSocket.java:226)\n",
      "  at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:266)\n",
      "  at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)\n",
      "  at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:204)\n",
      "  ... 83 more\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "  at java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "  at java.net.Socket.connect(Socket.java:607)\n",
      "  at org.apache.thrift.transport.TSocket.open(TSocket.java:221)\n",
      "  ... 86 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/** ***************************************************************\n",
    "앞에서 읽은 샘플 데이터를 Hudi 옵션에 맞춰서 S3에 Hudi Dataset 형식으로 저장합니다. \n",
    "비교적 시간이 걸리는 작업입니다. \n",
    "Lets write our input dataset to Hudi.\n",
    "*******************************************************************/\n",
    "(sourceData.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "\n",
    "  //Operation Key tells Hudi whether this is an Insert, Upsert, or Bulk Insert operation\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, \n",
    "                                   DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL)\n",
    "  \n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb3be5868a6413f8858af9ac9dc4202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimizedHudiViewDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n"
     ]
    }
   ],
   "source": [
    "/** **********************************\n",
    "Querying Hudi data is easy. We set the format to \"org.apache.hudi\"\n",
    "**************************************/\n",
    "val readOptimizedHudiViewDF = (spark.read\n",
    "       .format(\"org.apache.hudi\")\n",
    "       .load(hudiTablePath + \"/*/*/*/*\")\n",
    "       .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db6f04c260f44b8a8a0251c234ae07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "+-----------+--------+\n",
      "|star_rating|count(1)|\n",
      "+-----------+--------+\n",
      "|          1|  246630|\n",
      "|          2|  124792|\n",
      "|          3|  189490|\n",
      "|          4|  419488|\n",
      "|          5| 1660254|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/** ***********************************\n",
    "Lets take a look at our data. Lets say someone says there is something odd going \n",
    "on with star ratings.\n",
    "**************************************/\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_raw_ro_table\");\n",
    "spark.sql(\"\"\"select star_rating, count(*) from amazon_product_reviews_raw_ro_table \n",
    "                                group by star_rating order by star_rating ASC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc38bf4e8f6449f900b078642afe08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsertdf: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n"
     ]
    }
   ],
   "source": [
    "/** *********************************\n",
    "Select the rows we want to update and and make the update.\n",
    "************************************/\n",
    "val upsertdf = (readOptimizedHudiViewDF.filter($\"star_rating\" === 100)\n",
    "                                .withColumn(\"star_rating\", lit(null).cast(IntegerType)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67220709029a4ab0a25537f8c5229dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/** ********************************\n",
    "Before, if you wanted to update data in S3, you had to read the old data, merge with the new data, and then overwrite\n",
    "the old data. Now, with Hudi, you can directly update the data in-place.\n",
    "************************************/\n",
    "(upsertdf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, \n",
    "                    DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a56d066439448cc8dff4c3a970f44ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimizedHudiViewDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "+-----------+--------+\n",
      "|star_rating|count(1)|\n",
      "+-----------+--------+\n",
      "|          1|  246630|\n",
      "|          2|  124792|\n",
      "|          3|  189490|\n",
      "|          4|  419488|\n",
      "|          5| 1660254|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val readOptimizedHudiViewDF = (spark.read.format(\"org.apache.hudi\")\n",
    "                                    .load(hudiTablePath + \"/*/*/*/*\").cache())\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_ro_table\");\n",
    "\n",
    "spark.sql(\"\"\"select star_rating, count(*) from amazon_product_reviews_ro_table \n",
    "                        group by star_rating order by star_rating ASC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644a68e634e447c48d08507125ec139f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.apache.hudi.hive.HoodieHiveSyncException: Cannot create hive connection jdbc:hive2://localhost:10000/\n",
      "  at org.apache.hudi.hive.HoodieHiveClient.createHiveConnection(HoodieHiveClient.java:549)\n",
      "  at org.apache.hudi.hive.HoodieHiveClient.<init>(HoodieHiveClient.java:108)\n",
      "  at org.apache.hudi.hive.HiveSyncTool.<init>(HiveSyncTool.java:60)\n",
      "  at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:235)\n",
      "  at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:169)\n",
      "  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:91)\n",
      "  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "  ... 52 elided\n",
      "Caused by: java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000: java.net.ConnectException: Connection refused (Connection refused)\n",
      "  at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:232)\n",
      "  at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:176)\n",
      "  at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)\n",
      "  at java.sql.DriverManager.getConnection(DriverManager.java:664)\n",
      "  at java.sql.DriverManager.getConnection(DriverManager.java:247)\n",
      "  at org.apache.hudi.hive.HoodieHiveClient.createHiveConnection(HoodieHiveClient.java:546)\n",
      "  ... 78 more\n",
      "Caused by: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)\n",
      "  at org.apache.thrift.transport.TSocket.open(TSocket.java:226)\n",
      "  at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:266)\n",
      "  at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)\n",
      "  at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:204)\n",
      "  ... 83 more\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "  at java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "  at java.net.Socket.connect(Socket.java:607)\n",
      "  at org.apache.thrift.transport.TSocket.open(TSocket.java:221)\n",
      "  ... 86 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/** *******************************\n",
    "Now, suppose we need to delete a customers information due to GDPR because a \n",
    "request was made by a customer?\n",
    "***********************************/\n",
    "val deleteRowsDf = readOptimizedHudiViewDF.filter($\"customer_id\" === 32068341);\n",
    "\n",
    "//Deletion\n",
    "(deleteRowsDf.write\n",
    "  .format(\"org.apache.hudi\")\n",
    "  .options(hudiOptions)\n",
    "  //We set the operation to UPSERT\n",
    "  .option(DataSourceWriteOptions.OPERATION_OPT_KEY, \n",
    "                       DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL)\n",
    "  //We set the Payload Class to be empty record \n",
    "  .option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \n",
    "                       \"org.apache.hudi.EmptyHoodieRecordPayload\")\n",
    "  .mode(SaveMode.Append)\n",
    "  .save(hudiTablePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d7a7ef9de8479988e8a7fbc6ff8a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readOptimizedHudiViewDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val readOptimizedHudiViewDF = (spark.read.format(\"org.apache.hudi\")\n",
    "                               .load(hudiTablePath + \"/*/*/*/*\").cache())\n",
    "readOptimizedHudiViewDF.registerTempTable(\"amazon_product_reviews_ro_table\");\n",
    "\n",
    "spark.sql(\"\"\"select count(*) from amazon_product_reviews_ro_table where\n",
    "                                    customer_id = 32068341\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf17284a02a46f6ad76e45e5f395d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: Array[String] = Array(20200318090304)\n",
      "res32: String = [Ljava.lang.String;@4679d7e5\n"
     ]
    }
   ],
   "source": [
    "/**************************************\n",
    "We can also do point in time queries. Lets take a look at all the commits.\n",
    "***************************************/\n",
    "val commits = (spark.sql(\"\"\"select distinct(_hoodie_commit_time) as commitTime from \n",
    "                            amazon_product_reviews_ro_table order by commitTime\"\"\")\n",
    "                .map(k => k.getString(0)).take(50))\n",
    "commits.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc76625218140a0aef340d43753120e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginTime: String = 0\n",
      "endTime: String = 20200318090304\n",
      "amazon_product_reviews_table: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 19 more fields]\n",
      "+---------+----------+-------------+-----------+\n",
      "|review_id|product_id|product_title|star_rating|\n",
      "+---------+----------+-------------+-----------+\n",
      "+---------+----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/** ********************************\n",
    "Suppose that we wanted to know what was a review at a certain point of time or a range\n",
    "of commits.\n",
    "************************************/\n",
    "val beginTime = \"0\"\n",
    "val endTime = commits(0) // commit time we are interested in\n",
    "\n",
    "val amazon_product_reviews_table = (spark.read\n",
    "     .format(\"org.apache.hudi\")\n",
    "     //Mark that we want to do an incremental query\n",
    "     .option(DataSourceReadOptions.VIEW_TYPE_OPT_KEY, \n",
    "                             DataSourceReadOptions.VIEW_TYPE_INCREMENTAL_OPT_VAL)\n",
    "\n",
    "     //Set at what time we want to start quering.\n",
    "     .option(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY, beginTime)\n",
    "     .option(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, endTime)\n",
    "\n",
    "     .options(hudiOptions)\n",
    "     .load(hudiTablePath)).cache()\n",
    "\n",
    "(amazon_product_reviews_table.select(\"review_id\", \"product_id\", \"product_title\", \n",
    "                                   \"star_rating\").filter($\"star_rating\" === 100).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38896b230cce426c8e47ffe4df50b326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/***********************************\n",
    "Hive and Presto can query the data too!\n",
    "************************************/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
